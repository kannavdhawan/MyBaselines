{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q2kannav",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6qJDeV/R+JBCcnP0mt83K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kannavdhawan/commiting_Personal_baselines/blob/master/q2kannav.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfNeecX4q3if",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "e458cc52-0462-4285-8556-3e5a103aee9f"
      },
      "source": [
        "from keras.utils import get_file\n",
        "import tarfile\n",
        "data_dir = get_file('aclImdb_v1.tar.gz', 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', cache_subdir = \"datasets\",hash_algorithm = \"auto\", extract = True, archive_format = \"auto\")\n",
        "\n",
        "my_tar = tarfile.open(data_dir)\n",
        "my_tar.extractall('./data/') # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m_extract_archive\u001b[0;34m(file_path, path, archive_format)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                 except (tarfile.TarError, RuntimeError,\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtarinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2412\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2413\u001b[0;31m                 \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2414\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2296\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2297\u001b[0;31m                 \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtarfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFHeaderError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mfromtarfile\u001b[0;34m(cls, tarfile)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mfrombuf\u001b[0;34m(cls, buf, encoding, errors)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0mchksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m148\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m156\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mchksum\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcalc_chksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidHeaderError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad checksum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mcalc_chksums\u001b[0;34m(buf)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \"\"\"\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0munsigned_chksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"148B8x356B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0msigned_chksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"148b8x356b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-e21aee80d7ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aclImdb_v1.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_subdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"datasets\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhash_algorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_tar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0m_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m_extract_archive\u001b[0;34m(file_path, path, archive_format)\u001b[0m\n\u001b[1;32m    115\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6JI6nNDQ8RU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "36959839-7c62-46ea-f856-d801245bdf31"
      },
      "source": [
        "import os\n",
        "import keras \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "import timeit\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import io\n",
        "import json\n",
        "import nltk\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Embedding, Dropout, Activation, Flatten,Conv1D,MaxPooling1D\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords=stopwords.words('english')\n",
        "# import "
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMt5Mcbgw6G9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sorting_alpha(path):\n",
        "    conversion_lambda = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "    alphanumeric_keys = lambda keys: [ conversion_lambda(char) for char in re.split('([0-9]+)', keys) ]\n",
        "    sort_list=sorted(path, key=alphanumeric_keys)\n",
        "    return sort_list"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfNjf8pAvor2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path):\n",
        "  X=[]\n",
        "  y=[]\n",
        "  for sentiment in ['pos', 'neg']:\n",
        "      for file_name in sorting_alpha(os.listdir(os.path.join(data_path,sentiment))):\n",
        "          if file_name.endswith('.txt'):\n",
        "              with open(os.path.join(data_path,sentiment,file_name)) as f:\n",
        "                  X.append(f.read())\n",
        "              y.append(0 if sentiment == 'neg' else 1)\n",
        "  return X,y"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRdE8MuAf0Vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features,train_labels=load_data(os.path.join(\"/content/data\",'aclImdb','train'))\n",
        "test_features,test_labels=load_data(os.path.join(\"/content/data\",'aclImdb','test'))"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXz6C4BAjmb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "7ab43627-fb05-453d-a291-9f2661e86624"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Type train features:  <class 'list'>\n",
        "Length of train features:  25000\n",
        "Type test features:  <class 'list'>\n",
        "Length test features:  25000\n",
        "\"\"\"\n",
        "\n",
        "print(\"Type train features: \",type(train_features))\n",
        "print(\"Length of train features: \",len(train_features))\n",
        "print(\"Type test features: \",type(test_features))\n",
        "print(\"Length test features: \",len(test_features))\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type train features:  <class 'list'>\n",
            "Length of train features:  25000\n",
            "Type test features:  <class 'list'>\n",
            "Length test features:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jSTGjLcbcjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting into DataFrames\n",
        "train=pd.concat([pd.DataFrame(train_features,columns=['Text']),pd.DataFrame(train_labels,columns=['Label'])],axis=1)\n",
        "test=pd.concat([pd.DataFrame(test_features,columns=['Text']),pd.DataFrame(test_labels,columns=['Label'])],axis=1)\n",
        "assert train.shape==test.shape"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPyo3tIPcklg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "ffdefdad-b7e0-48e8-aace-4c6079623243"
      },
      "source": [
        "print(train.head(2))\n",
        "print(train.tail(2))\n",
        "print(test.head(2))\n",
        "print(test.tail(2))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Text  Label\n",
            "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
            "1  If you like adult comedy cartoons, like South ...      1\n",
            "                                                    Text  Label\n",
            "24998  This film has the kernel of a really good stor...      0\n",
            "24999  I went to the movie as a Sneak Preview in Aust...      0\n",
            "                                                Text  Label\n",
            "0  I went and saw this movie last night after bei...      1\n",
            "1  My boyfriend and I went to watch The Guardian....      1\n",
            "                                                    Text  Label\n",
            "24998  Oh, dear! This has to be one of the worst film...      0\n",
            "24999  This movie was sooo bad. It wasn't even funny ...      0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA6AASRC24NQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Finding all punctuations in train set..\n",
        "# set_punc=set()\n",
        "# for reviews in train['Text']:\n",
        "#   for chr in reviews:\n",
        "#     set_punc=set_punc.union(set(re.findall(r\"[^a-zA-Z0-9\\s]\",chr)))\n",
        "\n",
        "# print(\"Punctuations in our dataset: \",set_punc)\n",
        "\n",
        "# relevant punctuations updated from above list\n",
        "punctuations=\"[\\s.;:!\\'?,\\\"()\\[\\]/_ÉºèíÁ{()ïá`&₤%äÜô“êðßÈ³¾âø;Ø\\®»!-=ñÀöã?!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~å°@0¨ë:¢û*$´ùóüçúî~½<’æ‘§}£.«ÕÊì¤ÃÄ·,éý|-Åō#ò¿–à><]\""
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uu4lB9cXclS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def punctuation_remover(series,punctuations):\n",
        "    series_list=[re.sub(punctuations,\" \",row) for row in series]\n",
        "    return series_list\n",
        "\n",
        "def punctuation_remover_basic(list_text):\n",
        "    list_text=[re.sub(r'[^a-zA-Z0-9]+',\" \",review) for review in list_text]\n",
        "    list_text=[re.sub(r'\\s+',\" \",review) for review in list_text]\n",
        "    list_text=[re.sub(r'(<br\\s*/><br\\s*/>)|(\\-)|(\\/)',\" \",review) for review in list_text]\n",
        "    return list_text\n",
        "\n",
        "def lower_text(list_text):\n",
        "    list_text=[review.lower() for review in list_text]\n",
        "    return list_text\n",
        "\n",
        "def stopwords_removal(list_text,stopword_list):\n",
        "    list_text_nsw=[]\n",
        "    for review in list_text:\n",
        "        list_text_nsw.append(' '.join([word for word in review.split() if word not in stopword_list]))\n",
        "    return list_text_nsw\n",
        "\n",
        "def tokenize_word_sentences(list_text):       \n",
        "    tokenized_list = []\n",
        "    for review in list_text:\n",
        "        tokenized_list.append(word_tokenize(review))  # list of list \n",
        "          \n",
        "    return tokenized_list      \n",
        "\n",
        "def w2v_embedding(formatted_dataset):\n",
        "    print(\"Training...\")\n",
        "    start=timeit.default_timer()\n",
        "    w2v=Word2Vec(sentences=formatted_dataset,min_count=4, size=350,window=2,workers=4,iter=5) #sample=e-5, alpha=0.01,min_alpha=0.0001\n",
        "    stop=timeit.default_timer()\n",
        "    print(\"Time taken: \",stop-start)\n",
        "    w2v.save(\"word2vec.model\")\n",
        "    return w2v\n",
        "\n",
        "def most_sim(model,word,n):\n",
        "    print(\"\\n\\n---------------\",word,\": Most similar words---------------\")\n",
        "    try:\n",
        "        alltups=[]\n",
        "        \n",
        "        for i in range(n):\n",
        "            tup=model.most_similar(positive=[word], topn=n)[i]\n",
        "            alltups.append(tup)\n",
        "        for k, v in dict(alltups).items():\n",
        "            print (k, '-->', v)\n",
        "    except:\n",
        "        print(\"word : \",word,\" not in vocab\")\n",
        "\n",
        "def fit_on_text(data):\n",
        "    \n",
        "   \n",
        "    length_list=[len(seq) for seq in data]\n",
        "    avg=sum(length_list)/len(length_list)\n",
        "    max_length= int(avg+(max(length_list)-avg))# average number of words in each sentence.   \n",
        "    max_length=500 \n",
        "    print(\"--------------------------------------------------------------------------------------------------\",max_length)\n",
        "    \n",
        "    token=Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        " \n",
        "    list_of_strings_full_data=[' '.join(seq[:max_length]) for seq in data]\n",
        "    token.fit_on_texts(list_of_strings_full_data)\n",
        "    \n",
        "    tokenizer_json = token.to_json()\n",
        "    with io.open(os.path.join('tokenizer.json'), 'w', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "\n",
        "\n",
        "    return max_length,token\n",
        "\n",
        "def texts_to_sequences(token,max_length,X_train,X_test):\n",
        "    tr=[' '.join(seq[:max_length]) for seq in X_train]  #['This product is very good','']\n",
        "    # vl=[' '.join(seq[:max_length]) for seq in X_val]    #['This product is very good','']\n",
        "    tst=[' '.join(seq[:max_length]) for seq in X_test]  #['This product is very good','']\n",
        "    print(tr[0:1])\n",
        "    X_train = token.texts_to_sequences(tr)\n",
        "    # X_val = token.texts_to_sequences(vl)\n",
        "    X_test = token.texts_to_sequences(tst)\n",
        "\n",
        "    X_train = pad_sequences(X_train, maxlen=max_length, padding='post', truncating='post')\n",
        "    # X_val = pad_sequences(X_val, maxlen=max_length, padding='post', truncating='post')\n",
        "    X_test = pad_sequences(X_test, maxlen=max_length, padding='post', truncating='post')\n",
        "    print(\"shape test 1:\",X_train.shape)\n",
        "    print(X_train[0])\n",
        "    return X_train,X_test\n",
        "\n",
        "def embedding_matrix(token):\n",
        "\n",
        "    w2v_embeddings = Word2Vec.load('word2vec.model')\n",
        "   \n",
        "    e_dim=w2v_embeddings.vector_size #350\n",
        "\n",
        "    print(\"vector size embedding\",e_dim)\n",
        "\n",
        "\n",
        "    v_size=len(token.word_index)+1 \n",
        "\n",
        "    print(\"vocabulary_size: \",v_size)\n",
        "    \n",
        "    # making the embedding matrtix and feeding with array from word2vec embeddings.\n",
        "\n",
        "    embed_matrix=np.random.randn(v_size,e_dim) #114556*350\n",
        "    for word,index in token.word_index.items():\n",
        "        if word in w2v_embeddings.wv.vocab:\n",
        "            embed_matrix[index]=w2v_embeddings[word]#feeding the embedding matrix with array from word2vec embeddings\n",
        "        else:\n",
        "            embed_matrix[index]=np.random.randn(1,e_dim) # if word from word index is not there in word2vec embeddings, input randomly.\n",
        "    \n",
        "\n",
        "\n",
        "    return e_dim,v_size,embed_matrix\n",
        "\n",
        "\n",
        "def to_df(X_train,X_test,y_train,y_test):\n",
        "    \n",
        "\n",
        "    X_train=pd.DataFrame(X_train)\n",
        "    y_train=pd.DataFrame(y_train)\n",
        "    \n",
        "    X_test=pd.DataFrame(X_test)\n",
        "    y_test=pd.DataFrame(y_test)\n",
        "    \n",
        "    return X_train,X_test,y_train,y_test\n",
        "\n",
        "\n",
        "def model(X_train,X_test,max_length,e_dim,v_size,e_mat,y_train,y_test,act_func,l2_norm_f,l2_norm,dropout_f,dropout):\n",
        "\n",
        "    # Embedding layer\n",
        "    clf=Sequential()\n",
        "    e_layer=Embedding(input_dim=v_size,output_dim=e_dim,weights=[e_mat], input_length=max_length,\n",
        "                            trainable=False)                              #(114556,350,[114556*350],24)==>(None,24,350) i.e. (input_length,output_dim)\n",
        "    clf.add(e_layer)\n",
        "    #Flatten\n",
        "    clf.add(Flatten()) #flatten\n",
        "    #Dense\n",
        "    if l2_norm_f==True:\n",
        "        clf.add(Dense(128,activation=act_func,kernel_regularizer=regularizers.l2(l2_norm)))\n",
        "    else:\n",
        "        clf.add(Dense(128,activation=act_func))\n",
        "    #dropout\n",
        "    if dropout_f==True:\n",
        "        clf.add(Dropout(dropout))\n",
        "    else:\n",
        "        pass\n",
        "    clf.add(Dense(2,activation='softmax'))# final layer\n",
        "    clf.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "    # print(clf.summary())\n",
        "    \n",
        "    history=clf.fit(X_train, y_train,batch_size=1024,epochs=15)\n",
        "\n",
        "    test_score,test_acc = clf.evaluate(X_test,y_test,batch_size=1024)\n",
        "    print(\"Test Accuracy : \", test_acc*100)\n",
        "\n",
        "    return test_acc*100,history\n",
        "\n",
        "def model1(vocab_len, model_dim, weights_matrix, len_review, X_train, X_test, y_train, y_test):\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(input_dim = vocab_len, output_dim = model_dim, weights = [weights_matrix], input_length = len_review,trainable=True))\n",
        "\n",
        "  model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
        "\n",
        "  model.add(MaxPooling1D())\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(128, activation = 'relu'))\n",
        "\n",
        "  model.add(Dropout(rate = 0.3))\n",
        "\n",
        "  model.add(Dense(2,activation ='softmax'))\n",
        "\n",
        "  model.compile(loss ='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size = 100, epochs = 5)\n",
        "  \n",
        "  acc_score = model.evaluate(X_test,y_test)\n",
        "\n",
        "  print(\"Test Accuracy is {}% \".format(acc_score[1] * 100))\n",
        "  \n",
        "  model.save(\"m.model\")"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPMziLRXObJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CGSEn9Ha88E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing punctuations\n",
        "train_text=punctuation_remover(train['Text'],punctuations) # list\n",
        "train_text=punctuation_remover_basic(train_text)           # list\n",
        "\n",
        "# Removing punctuations additionally \n",
        "\n",
        "test_text=punctuation_remover(test['Text'],punctuations)\n",
        "test_text=punctuation_remover_basic(test_text)           # list\n",
        "\n",
        "# lowercasing\n",
        "\n",
        "train_text=lower_text(train_text)\n",
        "test_text=lower_text(test_text)\n",
        "\n",
        "# Stopword removal\n",
        "stopword_list_1=[ 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",'i', 'me', 'my', 'myself', \"you've\", \"you'll\", \"you'd\", 'your',\n",
        "                     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself',\n",
        "                      'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves']\n",
        "stopword_list_2=[\"a\", \"about\", 'other', 'some', 'such',  'own', 'same', 'so', 'than','s',\n",
        "                          'too', 'to', 'from', 'in', 'out', 'on', 'off', 'over', 'under', \n",
        "                        'then', 'once', 'here', 'there', 'all', 'any', 'both', 'each',\n",
        "                          't', 'can', 'will', 'just', 'don', 'now', 'd','s']\n",
        "\n",
        "stopword_list=list(set(stopword_list_1).union(set(stopword_list_2)))\n",
        "\n",
        "train_text=stopwords_removal(train_text,stopword_list)\n",
        "test_text=stopwords_removal(test_text,stopword_list)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XDbtt-yFIvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_text[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv-GpHyrmvoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text_tokenized=tokenize_word_sentences(train_text)\n",
        "test_text_tokenized=tokenize_word_sentences(test_text)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH_JfFnhf_IQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1a2117d1-d235-4ea3-d41f-2844fa43d545"
      },
      "source": [
        "w2v=w2v_embedding(train_text_tokenized)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Time taken:  62.813864585000374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV0_YeIMh7vt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c3b103ae-82f2-4a02-80e1-23b2180a9d12"
      },
      "source": [
        "w2v=Word2Vec.load('word2vec.model') "
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWVnVpVqkE6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "2537aeaf-8b31-4e1f-f52c-be053eb4914c"
      },
      "source": [
        "most_sim(w2v,\"good\",20)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--------------- good : Most similar words---------------\n",
            "decent --> 0.7245098352432251\n",
            "fine --> 0.6855379343032837\n",
            "great --> 0.6772593259811401\n",
            "bad --> 0.6638052463531494\n",
            "cool --> 0.649214506149292\n",
            "nice --> 0.6345492601394653\n",
            "solid --> 0.6197190284729004\n",
            "excellent --> 0.6182861328125\n",
            "terrific --> 0.6178426742553711\n",
            "fantastic --> 0.6020016670227051\n",
            "ok --> 0.6013531684875488\n",
            "impressive --> 0.5881668925285339\n",
            "interesting --> 0.5815615057945251\n",
            "funny --> 0.5760958790779114\n",
            "awesome --> 0.5756717920303345\n",
            "lousy --> 0.5749622583389282\n",
            "fair --> 0.5739405751228333\n",
            "mediocre --> 0.5730897784233093\n",
            "alright --> 0.5657459497451782\n",
            "scary --> 0.5645594000816345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5BzkcODyq11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7ca098f9-e5d0-45b7-8621-f5d6045f0708"
      },
      "source": [
        "max_length,token=fit_on_text(train_text_tokenized)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------------- 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr3Wqj1dzjgd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "3121a821-cbf0-4937-e1dd-70e4d79c57e7"
      },
      "source": [
        "X_train,X_test=texts_to_sequences(token,max_length,train_text_tokenized,test_text_tokenized)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bromwell high is cartoon comedy ran at the time as programs school life as teachers years the teaching profession lead believe that bromwell high satire is much closer reality is teachers the scramble survive financially the insightful students who see right through pathetic teachers pomp the pettiness of the whole situation remind of the schools knew and students when saw the episode which student repeatedly tried burn down the school immediately recalled at high classic line inspector m sack one of teachers student welcome bromwell high expect that many adults of age think that bromwell high is far fetched what pity that isn']\n",
            "shape test 1: (25000, 500)\n",
            "[20466   255     4   983   159  2086    20     1    34     9  5751   324\n",
            "    74     9  4990   109     1  4804  5752   415   210     7 20466   255\n",
            "  1883     4    46  2323   566     4  4990     1 23538  1903  9810     1\n",
            "  5816  1412    23    42   158   102  1139  4990 19364     1 45538     3\n",
            "     1   175   833  3014     3     1  5564   621     2  1412    30   164\n",
            "     1   328    39  1338  3598   744  3449   141     1   324  1151 13152\n",
            "    20   255   301   291  2921    98  7535    19     3  4990  1338  2255\n",
            " 20466   255   473     7    73  1387     3   485    65     7 20466   255\n",
            "     4   182  4018    28  2144     7   169     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m5fjbTezjjn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "c002741d-5240-4b76-9baa-88229d39639b"
      },
      "source": [
        "e_dim,v_size,embed_matrix=embedding_matrix(token)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vector size embedding 350\n",
            "vocabulary_size:  72096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:105: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLGdhVIQ5IMZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b238d79a-1b99-4521-bf2f-068bc0ebac93"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 500)\n",
            "(25000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdIUwpfn5SgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Converting into categorical data \n",
        "     \n",
        "y_train=np_utils.to_categorical(train['Label'])\n",
        "y_test=np_utils.to_categorical(test['Label'])"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-95cADd5TAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=to_df(X_train,X_test,y_train,y_test)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug018fYw_zG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBnC8xwr5TG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "668bd368-6c7b-470e-a8ef-a5178afe04c1"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrRGVnYh6NUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "fa5f0e08-21a3-4256-9bfd-ca680b5e5f9b"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdj4KRq96N7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a4c8199f-e860-4310-8847-1a2287657a89"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0    1\n",
              "0      0.0  1.0\n",
              "1      0.0  1.0\n",
              "2      0.0  1.0\n",
              "3      0.0  1.0\n",
              "4      0.0  1.0\n",
              "...    ...  ...\n",
              "24995  1.0  0.0\n",
              "24996  1.0  0.0\n",
              "24997  1.0  0.0\n",
              "24998  1.0  0.0\n",
              "24999  1.0  0.0\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgd5qF3A6TzU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "55400247-b581-4b04-b958-5860b672e756"
      },
      "source": [
        "acc1,history1=model(X_train,X_test,max_length,e_dim,v_size,embed_matrix,y_train,y_test,'relu',False,0,False,0) # dropout=False | l2 False"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "25000/25000 [==============================] - 3s 113us/step - loss: 6.9322 - accuracy: 0.5130\n",
            "Epoch 2/15\n",
            "25000/25000 [==============================] - 3s 102us/step - loss: 7.1232 - accuracy: 0.5312\n",
            "Epoch 3/15\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 7.0905 - accuracy: 0.5352\n",
            "Epoch 4/15\n",
            "25000/25000 [==============================] - 3s 102us/step - loss: 7.2293 - accuracy: 0.5293\n",
            "Epoch 5/15\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 7.1656 - accuracy: 0.5331\n",
            "Epoch 6/15\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 7.1271 - accuracy: 0.5334\n",
            "Epoch 7/15\n",
            "25000/25000 [==============================] - 3s 102us/step - loss: 7.0331 - accuracy: 0.5388\n",
            "Epoch 8/15\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 7.0092 - accuracy: 0.5404\n",
            "Epoch 9/15\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 6.9873 - accuracy: 0.5426\n",
            "Epoch 10/15\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 7.0117 - accuracy: 0.5418\n",
            "Epoch 11/15\n",
            "25000/25000 [==============================] - 3s 102us/step - loss: 6.9107 - accuracy: 0.5484\n",
            "Epoch 12/15\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 6.9628 - accuracy: 0.5433\n",
            "Epoch 13/15\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 6.9417 - accuracy: 0.5443\n",
            "Epoch 14/15\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 6.9257 - accuracy: 0.5457\n",
            "Epoch 15/15\n",
            "25000/25000 [==============================] - 3s 101us/step - loss: 6.8749 - accuracy: 0.5483\n",
            "25000/25000 [==============================] - 1s 54us/step\n",
            "Test Accuracy :  51.819998025894165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDe0s0OR6T2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5d62c6bb-ff3e-477d-e2c0-77809d5c5732"
      },
      "source": [
        "model1(v_size, e_dim, embed_matrix, max_length, X_train, X_test, y_train, y_test)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "25000/25000 [==============================] - 12s 495us/step - loss: 0.6859 - accuracy: 0.6225\n",
            "Epoch 2/5\n",
            "25000/25000 [==============================] - 12s 490us/step - loss: 0.2902 - accuracy: 0.8853\n",
            "Epoch 3/5\n",
            "25000/25000 [==============================] - 12s 487us/step - loss: 0.1311 - accuracy: 0.9544\n",
            "Epoch 4/5\n",
            "25000/25000 [==============================] - 12s 491us/step - loss: 0.0450 - accuracy: 0.9870\n",
            "Epoch 5/5\n",
            "25000/25000 [==============================] - 12s 488us/step - loss: 0.0151 - accuracy: 0.9960\n",
            "25000/25000 [==============================] - 4s 164us/step\n",
            "Test Accuracy is 86.10399961471558% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrEq5wit6T7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyXvZ38e0Brw",
        "colab_type": "text"
      },
      "source": [
        "- Remove punctuations\n",
        "- Remove stopwords\n",
        "- Lemmatization\n",
        "- Tokenize\n",
        "- embedding word2vec \n",
        "- fit on text keras\n",
        "- Text  to seq keras \n",
        "- pad seq keras \n",
        "- embed mat manually \n",
        "- Model keras conv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- To check for accuracy\n",
        "  - stopword list "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE-MtRvLvh4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 98,
      "outputs": []
    }
  ]
}